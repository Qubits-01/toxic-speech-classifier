from typing import Union

from fastapi import FastAPI

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

import joblib

app = FastAPI()

# [start] INITIALIZATIONS NEEDED FOR THE AI MODEL. ==============================================================================================
# 1. Initialize the custom stopwords.
# Built-in English stop words.
english_stop_words = CountVectorizer(stop_words="english").get_stop_words()

# Generated by ChatGPT.
filipino_stop_words1 = [
    "ako",
    "akin",
    "ako'y",
    "amin",
    "aming",
    "ang",
    "ano",
    "anuman",
    "apat",
    "at",
    "atin",
    "ating",
    "ay",
    "bababa",
    "bago",
    "bakit",
    "bawat",
    "bilang",
    "dahil",
    "dalawa",
    "dapat",
    "din",
    "dito",
    "doon",
    "gagawin",
    "gayunman",
    "ginagawa",
    "ginawa",
    "ginawang",
    "gumawa",
    "gusto",
    "habang",
    "hanggang",
    "hindi",
    "huwag",
    "iba",
    "ibaba",
    "ibabaw",
    "ibig",
    "ikaw",
    "ilagay",
    "ilalim",
    "ilan",
    "inyong",
    "isa",
    "isang",
    "ito",
    "iyo",
    "iyon",
    "iyong",
    "ka",
    "kahit",
    "kailangan",
    "kailanman",
    "kami",
    "kanila",
    "kanilang",
    "kanino",
    "kanya",
    "kanyang",
    "kapag",
    "kapwa",
    "karamihan",
    "katiyakan",
    "katulad",
    "kay",
    "kaya",
    "kaysa",
    "ko",
    "kung",
    "laban",
    "lahat",
    "lamang",
    "likod",
    "lima",
    "maaari",
    "maaaring",
    "maging",
    "mahusay",
    "makita",
    "marami",
    "marapat",
    "mga",
    "minsan",
    "mismo",
    "mula",
    "muli",
    "na",
    "nabanggit",
    "naging",
    "nagkaroon",
    "nais",
    "nakita",
    "namin",
    "napaka",
    "narito",
    "nasaan",
    "ng",
    "nga",
    "ngayon",
    "ni",
    "nila",
    "nilang",
    "nito",
    "niya",
    "niyang",
    "noon",
    "o",
    "pag",
    "pala",
    "para",
    "pati",
    "pero",
    "pumunta",
    "pumupunta",
    "sa",
    "saan",
    "sabi",
    "sabihin",
    "sarili",
    "si",
    "sila",
    "sino",
    "siya",
    "tatlo",
    "tayo",
    "tulad",
    "tungkol",
    "una",
    "walang",
]

# From an open-source GitHub repo (ref: https://github.com/stopwords-iso/stopwords-tl).
filipino_stop_words2 = [
    "ako",
    "sa",
    "akin",
    "ko",
    "aking",
    "sarili",
    "kami",
    "atin",
    "ang",
    "aming",
    "amin",
    "ating",
    "ka",
    "iyong",
    "iyo",
    "inyong",
    "siya",
    "kanya",
    "mismo",
    "ito",
    "nito",
    "kanyang",
    "sila",
    "nila",
    "kanila",
    "kanilang",
    "kung",
    "ano",
    "alin",
    "sino",
    "kanino",
    "na",
    "mga",
    "iyon",
    "am",
    "ay",
    "maging",
    "naging",
    "mayroon",
    "may",
    "nagkaroon",
    "pagkakaroon",
    "gumawa",
    "ginagawa",
    "ginawa",
    "paggawa",
    "ibig",
    "dapat",
    "maaari",
    "marapat",
    "kong",
    "ikaw",
    "tayo",
    "hindi",
    "namin",
    "gusto",
    "nais",
    "niyang",
    "nilang",
    "niya",
    "huwag",
    "ginawang",
    "gagawin",
    "maaaring",
    "sabihin",
    "narito",
    "kapag",
    "ni",
    "nasaan",
    "bakit",
    "paano",
    "kailangan",
    "walang",
    "katiyakan",
    "isang",
    "at",
    "pero",
    "o",
    "dahil",
    "bilang",
    "hanggang",
    "habang",
    "ng",
    "pamamagitan",
    "para",
    "tungkol",
    "laban",
    "pagitan",
    "panahon",
    "bago",
    "pagkatapos",
    "itaas",
    "ibaba",
    "mula",
    "pataas",
    "pababa",
    "palabas",
    "ibabaw",
    "ilalim",
    "muli",
    "pa",
    "minsan",
    "dito",
    "doon",
    "saan",
    "lahat",
    "anumang",
    "kapwa",
    "bawat",
    "ilan",
    "karamihan",
    "iba",
    "tulad",
    "lamang",
    "pareho",
    "kaya",
    "kaysa",
    "masyado",
    "napaka",
    "isa",
    "bababa",
    "kulang",
    "marami",
    "ngayon",
    "kailanman",
    "sabi",
    "nabanggit",
    "din",
    "kumuha",
    "pumunta",
    "pumupunta",
    "ilagay",
    "makita",
    "nakita",
    "katulad",
    "mahusay",
    "likod",
    "kahit",
    "paraan",
    "noon",
    "gayunman",
    "dalawa",
    "tatlo",
    "apat",
    "lima",
    "una",
    "pangalawa",
]

# Get the union of the two filipino stop words lists.
filipino_stop_words = list(set(filipino_stop_words1).union(set(filipino_stop_words2)))

# Get the union of the english and filipino stop words lists.
custom_stop_words = list(english_stop_words.union(filipino_stop_words))

# 2. Initialize the CountVectorizer.
cvect = CountVectorizer(
    stop_words=custom_stop_words,
    ngram_range=(1, 2),
    max_df=0.2,
    min_df=2,
    lowercase=False,
    binary=True,
)

# 3. Load the X_train data from disk.
X_train = pd.read_csv("X_train.csv")

# 4. Fit the X_train data to the CountVectorizer.
cvect.fit_transform(X_train["text"])

# 5. Load the Monomial Naive Bayes model from disk.
filename = "mnb_model.joblib"
mnb_model = joblib.load(filename)

# [end] INITIALIZATIONS NEEDED FOR THE AI MODEL. ================================================================================================


# Sample endpoint.
@app.get("/")
def read_root():
    return {"Hello": "World"}


# Sample endpoint with a query.
@app.get("/items/{item_id}")
def read_item(item_id: int, q: Union[str, None] = None):
    return {"item_id": item_id, "q": q}


# AIaaS endpoint.
@app.get("/is-text-toxic/")
def is_text_toxic(text: Union[str, None] = None):
    if text is None:
        return {"text": "[ No input text provided. ]"}

    # Vectorize the input text (i.e., convert it to a document-term-matrix).
    X_dtm = cvect.transform([text])

    # Make prediction.
    prediction = mnb_model.predict(X_dtm)
    probability = mnb_model.predict_proba(X_dtm)

    print("\n")
    print("[text]:", text)
    print("[isToxic]:", bool(prediction))
    print("[non_toxic_prob]:", probability[0][0])
    print("[toxic_prob]:", probability[0][1])
    print("\n")

    return {
        "text": text,
        "is_toxic": bool(prediction[0]),
        "non_toxic_probability": probability[0][0],
        "toxic_probability": probability[0][1],
    }
